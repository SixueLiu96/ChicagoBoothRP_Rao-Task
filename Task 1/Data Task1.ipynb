{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a script to collect all websites mentioned in the two PDFs, Part15 PX2 Declaration WIllis GoDaddy.pdf and Domains (Name.com).pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import idna \n",
    "import uritools \n",
    "import appdirs\n",
    "import urlextract\n",
    "from urlextract import URLExtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, I will use Python url extractor to help me extract urls from the original PDF. The following is a small example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GoDaddy.com']\n"
     ]
    }
   ],
   "source": [
    "# Build extractor object to extract url from string  \n",
    "\n",
    "extractor = URLExtract()\n",
    "urls = extractor.find_urls('in the custody of GoDaddy.com, Inc')\n",
    "print(urls) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pdf object for Domains.pdf \n",
    "\n",
    "pdfFileObj = open('Domains.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pdf contains 26 pages in total.\n"
     ]
    }
   ],
   "source": [
    "print('This pdf contains {} pages in total.'.format(pdfReader.numPages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res is a list that contains website urls from each page \n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(i)\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(pageObj.extractText())\n",
    "    res.append(urls[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is no urls extracted from Domains.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pdf contains 26 pages in total.\n"
     ]
    }
   ],
   "source": [
    "# Build pdf object for Part15 PX2 Declaration WIllis GoDaddy.pdf \n",
    "\n",
    "pdfFileObj = open('Part15 PX2 Declaration WIllis GoDaddy.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "print('This pdf contains {} pages in total.'.format(pdfReader.numPages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res is a list that contains website urls from each page \n",
    "\n",
    "res = []\n",
    "\n",
    "for i in range(pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(i)\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(pageObj.extractText())\n",
    "    res.append(urls[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is no urls extracted from Domains.pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I manually check the two PDFs, I find there are acutally some website urls that cannot be extracted by the script. For example, the page 3-6 in WIllis GoDaddy.pdf contains some webiste urls. I think the script cannot extract the urls for two reasons: the first one is the pdf is vertically wrote instead of horizontally like what we normal read. The second one is PyPDF2.PdfFileReader is not suitable for reading the information of certain pdfs. I am still trying to work on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
